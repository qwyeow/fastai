{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.io import *\n",
    "from fastai.conv_learner import *\n",
    "\n",
    "from fastai.column_data import *\n",
    "\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to download the collected works of Nietzsche to use as our data for this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pathlib to create a path object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH= Path('data/nietzsche/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data using `urllib:urlretrieve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urlretrieve(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", PATH/'nietzsche.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open path object and read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not ground\\nfor suspecting that all philosophers, in so far as they have been\\ndogmatists, have failed to understand women--that the terrible\\nseriousness and clumsy importunity with which they have usually paid\\ntheir addresses to Truth, have been unskilled and unseemly methods for\\nwinning a woman? Certainly she has never allowed herself to be won; and\\nat present every kind of dogma stands with sad and discouraged mien--IF,\\nindeed, it stands at all! For there are scoffers who maintain that it\\nhas fallen, that all dogma lies on the ground--nay more, that it is at\\nits last gasp. But to speak seriously, there are good grounds for hoping\\nthat all dogmatizing in philosophy, whatever solemn, whatever conclusive\\nand decided airs it has assumed, may have been only a noble puerilism\\nand tyronism; and probably the time is at hand when it will be once\\nand again understood WHAT has actually sufficed for the basis of such\\nimposing and abso'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = (PATH/'nietzsche.txt').open().read()\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print it in a friendier format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFACE\n",
      "\n",
      "\n",
      "SUPPOSING that Truth is a woman--what then? Is there not ground\n",
      "for suspecting that all philosophers, in so far as they have been\n",
      "dogmatists, have failed to understand women--that the terrible\n",
      "seriousness and clumsy importunity with which they have usually paid\n",
      "their addresses to Truth, have been unskilled and unseemly methods for\n",
      "winning a woman? Certainly she has never allowed herself to be won; and\n",
      "at present every kind of dogma stands with sad and discouraged mien--IF,\n",
      "indeed, it stands at all! For there are scoffers who maintain that it\n",
      "has fallen, that all dogma lies on the ground--nay more, that it is at\n",
      "its last gasp. But to speak seriously, there are good grounds for hoping\n",
      "that all dogmatizing in philosophy, whatever solemn, whatever conclusive\n",
      "and decided airs it has assumed, may have been only a noble puerilism\n",
      "and tyronism; and probably the time is at hand when it will be once\n",
      "and again understood WHAT has actually sufficed for the basis of such\n",
      "imposing and abso\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get length of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dictionary. Make a space for zero value = padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 85\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's useful to have a zero value in the dataset, e.g. for padding.  \n",
    "Insert it at position 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\x00', '\\n', ' ', '!']\n"
     ]
    }
   ],
   "source": [
    "chars.insert(0, \"\\0\")\n",
    "print(chars[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00\\n !\"\\'(),-.0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyzÆäæéë'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map from chars to indices and back again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = {c: i for i, c in enumerate(chars)}\n",
    "indices_char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*idx* will be the data we use from now on - it simply converts all the characters to their index (based on the mapping above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 42, 29, 30, 25, 27, 29, 1, 1, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [char_indices[c] for c in text]\n",
    "\n",
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not gro'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a list of every 4th character, starting at the 0th, 1st, 2nd, then 3rd characters  \n",
    "> c1_dat = 1st, 4th, 7th, 10th,..   \n",
    "> c2_dat = 2nd, 5th, 8th, 11th,..  \n",
    "> c3_dat = 3rd, 6th, 9th, 12th,..  \n",
    "> c4_dat = 4th, 7th, 10th, 13th,..  \n",
    "  \n",
    "Note that c1_dat and c4_dat are exactly the same, except the latter lags the former by one element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cs=3\n",
    "c1_dat = [idx[i]   for i in range(0, len(idx)-cs, cs)]\n",
    "c2_dat = [idx[i+1] for i in range(0, len(idx)-cs, cs)]\n",
    "c3_dat = [idx[i+2] for i in range(0, len(idx)-cs, cs)]\n",
    "c4_dat = [idx[i+3] for i in range(0, len(idx)-cs, cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sequence: [40, 42, 29, 30, 25, 27, 29, 1, 1, 1, 43, 45, 40, 40, 39, 43, 33, 38, 31, 2]\n",
      "\n",
      "\n",
      "c1_dat: 1st, 4th, 7th, 10th,.. [40, 30, 29, 1, 40, 43, 31, 61, 2, 74, 2, 2, 76, 54, 9, 54, 73, 67, 33, 73]\n",
      "c2_dat: 2nd, 5th, 8th, 11th,.. [42, 25, 1, 43, 40, 33, 2, 54, 44, 73, 62, 54, 68, 67, 76, 73, 61, 24, 72, 61]\n",
      "c3_dat: 3rd, 6th, 9th, 12th,.. [29, 27, 1, 45, 39, 38, 73, 73, 71, 61, 72, 2, 66, 9, 61, 2, 58, 2, 2, 58]\n",
      "c4_dat: 4th, 7th, 10th, 13th,.. [30, 29, 1, 40, 43, 31, 61, 2, 74, 2, 2, 76, 54, 9, 54, 73, 67, 33, 73, 71]\n"
     ]
    }
   ],
   "source": [
    "print(\"original sequence:\",idx[:20])\n",
    "print(\"\\n\")\n",
    "print(\"c1_dat: 1st, 4th, 7th, 10th,..\", c1_dat[:20]) \n",
    "print(\"c2_dat: 2nd, 5th, 8th, 11th,..\", c2_dat[:20])\n",
    "print(\"c3_dat: 3rd, 6th, 9th, 12th,..\", c3_dat[:20])\n",
    "print(\"c4_dat: 4th, 7th, 10th, 13th,..\", c4_dat[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.stack\n",
    "\n",
    "Signature: np.stack(arrays, axis=0, out=None)\n",
    "Docstring:\n",
    "Join a sequence of arrays along a new axis.\n",
    "\n",
    "The `axis` parameter specifies the index of the new axis in the dimensions\n",
    "of the result. For example, if ``axis=0`` it will be the first dimension\n",
    "and if ``axis=-1`` it will be the last dimension.\n",
    "\n",
    ".. versionadded:: 1.10.0\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "arrays : sequence of array_like\n",
    "    Each array must have the same shape.\n",
    "axis : int, optional\n",
    "    The axis in the result array along which the input arrays are stacked.\n",
    "out : ndarray, optional\n",
    "    If provided, the destination to place the result. The shape must be\n",
    "    correct, matching that of what stack would have returned if no\n",
    "    out argument were specified.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "stacked : ndarray\n",
    "    The stacked array has one more dimension than the input arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of five 3X4 numpy matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.26707,  0.53502, -0.36971,  0.47062],\n",
       "        [ 1.3231 ,  0.81272,  1.05069, -0.97046],\n",
       "        [-0.57811, -1.07036, -0.79336, -1.13682]]),\n",
       " array([[-0.40418,  0.00136, -1.92031,  0.14098],\n",
       "        [-1.96263, -0.35439, -0.54317, -0.1095 ],\n",
       "        [ 1.00624,  1.12282,  0.30095, -0.82838]]),\n",
       " array([[-0.13569, -0.52423, -0.34952,  2.70086],\n",
       "        [-0.74903, -0.89188, -0.78355,  0.95596],\n",
       "        [ 0.08061,  1.39371, -1.01446, -0.22409]]),\n",
       " array([[-1.43728,  1.07395, -0.59071, -0.58609],\n",
       "        [ 0.05319,  0.42956, -2.23133,  1.8199 ],\n",
       "        [-1.23453,  0.03955,  0.15082, -0.04645]]),\n",
       " array([[ 0.92975,  0.86156,  0.38327,  0.56463],\n",
       "        [-0.86046,  1.11132, -0.11699, -0.49987],\n",
       "        [ 0.25563,  1.15763, -1.07207,  1.5115 ]])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrays = [np.random.randn(3, 4) for _ in range(5)];arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np. stack will stack the five lists based on the axis specified.\n",
    "\n",
    "Examples\n",
    "--------\n",
    ">>> arrays = [np.random.randn(3, 4) for _ in range(5)]\n",
    ">>> np.stack(arrays, axis=0).shape\n",
    "(5, 3, 4)\n",
    "\n",
    ">>> np.stack(arrays, axis=1).shape\n",
    "(3, 5, 4)\n",
    "\n",
    ">>> np.stack(arrays, axis=2).shape\n",
    "(3, 4, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case, np.stack will just stack the only axis (axis=0)  \n",
    "\n",
    "At this point, `c1_dat` is still a single list, so np.stack will convert it to a 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40, 30, 29, ..., 72, 59, 67])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(c1_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x1 = np.stack(c1_dat)\n",
    "x2 = np.stack(c2_dat)\n",
    "x3 = np.stack(c3_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we put these 3 in a list, there will be 3 arrays of one column in the list.  \n",
    "Stacking in axis 0 will create a 3 X 10 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([40, 30, 29,  1, 40, 43, 31, 61,  2, 74]),\n",
       " array([42, 25,  1, 43, 40, 33,  2, 54, 44, 73]),\n",
       " array([29, 27,  1, 45, 39, 38, 73, 73, 71, 61])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x1[:10],x2[:10],x3[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 30, 29,  1, 40, 43, 31, 61,  2, 74],\n",
       "       [42, 25,  1, 43, 40, 33,  2, 54, 44, 73],\n",
       "       [29, 27,  1, 45, 39, 38, 73, 73, 71, 61]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([x1[:10],x2[:10],x3[:10]], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the 3 inputs are stacked along axis 1, it would look like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 42, 29],\n",
       "       [30, 25, 27],\n",
       "       [29,  1,  1],\n",
       "       [ 1, 43, 45],\n",
       "       [40, 40, 39],\n",
       "       [43, 33, 38],\n",
       "       [31,  2, 73],\n",
       "       [61, 54, 73],\n",
       "       [ 2, 44, 71],\n",
       "       [74, 73, 61]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stackedx = np.stack([x1[:10],x2[:10],x3[:10]], axis=1)\n",
    "stackedx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "c4_dat will be y, our output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 29,  1, 40, 43, 31, 61,  2, 74,  2,  2, 76, 54,  9, 54, 73, 67, 33, 73, 71])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.stack(c4_dat)\n",
    "y[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first 4 inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200297,), (200297,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a size for our hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of latent factors to create (i.e. the size of the embedding matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create zero matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how to create a zero matrix in torch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       "[torch.FloatTensor of size 3x5]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((3,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can put it on the GPU by using `.cuda()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.7996  0.1724  0.3232  0.6234\n",
       " 0.0993  0.6973  0.5799  0.0115\n",
       " 0.0138  0.9245  0.8453  0.4741\n",
       "[torch.cuda.FloatTensor of size 3x4 (GPU 0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(np.random.rand(3,4)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or put it on the GPU by turning it into a variable using V()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       " 0  0  0  0  0\n",
       "[torch.cuda.FloatTensor of size 3x5 (GPU 0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V(torch.zeros((3,5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47917, -0.18566, -1.10633, -1.19621],\n",
       "       [ 0.81253,  1.35624, -0.07201,  1.00353],\n",
       "       [ 0.36164, -0.64512,  0.3614 ,  1.53804]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.random.randn(3, 4)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim = 0 means the outermost square bracket, axis 0. The softmax is done down each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1437  0.1586  0.1227  0.0393\n",
       " 0.5230  0.7412  0.3451  0.3549\n",
       " 0.3332  0.1002  0.5323  0.6057\n",
       "[torch.cuda.FloatTensor of size 3x4 (GPU 0)]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(V(w), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logging the softmax. Note that all values are negative, the smaller the value, the more negative it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-1.9398 -1.8414 -2.0983 -3.2356\n",
       "-0.6481 -0.2995 -1.0640 -1.0358\n",
       "-1.0990 -2.3008 -0.6306 -0.5013\n",
       "[torch.cuda.FloatTensor of size 3x4 (GPU 0)]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(V(w), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`negative log softmax` will multiply the above by -1, making all values positive. The smaller the value, the higher the log(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the 3-character RNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Char3Model(nn.Module): \n",
    "    \n",
    "    \n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        \n",
    "        # embedding layer\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "\n",
    "        # input weight dense matrix\n",
    "        self.l_in = nn.Linear(n_fac, n_hidden)\n",
    "\n",
    "        # state weight dense matrix\n",
    "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        # output weight matrix\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, c1, c2, c3):\n",
    "                     \n",
    "        # initial zero hidden layer\n",
    "        # 1st rnn layer\n",
    "        in1 = F.relu(self.l_in(self.e(c1)))\n",
    "        h = V(torch.zeros(in1.size()).cuda())  # Note the cuda()\n",
    "        #h = V(torch.zeros(in1.size()))         \n",
    "        h = F.tanh(self.l_hidden(h+in1))\n",
    "         \n",
    "        # 2nd rnn layer        \n",
    "        in2 = F.relu(self.l_in(self.e(c2)))\n",
    "        h = F.tanh(self.l_hidden(h+in2))\n",
    "        \n",
    "        # 3rd rnn layer        \n",
    "        in3 = F.relu(self.l_in(self.e(c3)))\n",
    "        h = F.tanh(self.l_hidden(h+in3))\n",
    "        \n",
    "        return F.log_softmax(self.l_out(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "??ColumnarModelData.from_arrays\n",
    "\n",
    "Signature: ColumnarModelData.from_arrays(path, val_idxs, xs, y, is_reg=True, \n",
    "                                         is_multi=False, bs=64, test_xs=None, shuffle=True)\n",
    "Docstring: <no docstring>\n",
    "Source:   \n",
    "    @classmethod\n",
    "    def from_arrays(cls, path, val_idxs, xs, y, is_reg=True, is_multi=False, \n",
    "                    bs=64, test_xs=None, shuffle=True):\n",
    "        ((val_xs, trn_xs), (val_y, trn_y)) = split_by_idx(val_idxs, xs, y)\n",
    "        test_ds = PassthruDataset(*(test_xs.T), [0] * len(test_xs), is_reg=is_reg, \n",
    "                                  is_multi=is_multi) if test_xs is not None else None\n",
    "        return cls(path, PassthruDataset(*(trn_xs.T), trn_y, is_reg=is_reg, is_multi=is_multi),\n",
    "                   PassthruDataset(*(val_xs.T), val_y, is_reg=is_reg, is_multi=is_multi),\n",
    "                   bs=bs, shuffle=shuffle, test_ds=test_ds)\n",
    "File:      ~/Documents/Git/fastai/courses/dl1/fastai/column_data.py\n",
    "Type:      method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassthruDataset(Dataset):\n",
    "    def __init__(self,*args, is_reg=True, is_multi=False):\n",
    "        *xs,y=args\n",
    "        self.xs,self.y = xs,y\n",
    "        self.is_reg = is_reg\n",
    "        self.is_multi = is_multi\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return [o[idx] for o in self.xs] + [self.y[idx]]\n",
    "\n",
    "    @classmethod\n",
    "    def from_data_frame(cls, df, cols_x, col_y, is_reg=True, is_multi=False):\n",
    "        cols = [df[o] for o in cols_x+[col_y]]\n",
    "        return cls(*cols, is_reg=is_reg, is_multi=is_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the dataloader. The last row, just a single row, is used as validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_arrays(path = '.',\n",
    "                                   val_idxs = [-1], \n",
    "                                   xs = np.stack([x1,x2,x3], axis=1), \n",
    "                                   y =y, \n",
    "                                   bs=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine some methods available under the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of batches in dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.trn_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ???\n",
    "\n",
    "number of training rows X batch size = 392X512 = 200704  \n",
    "but len of dataset is 200295 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in training ds the x and y are joined together "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[40, 42, 29, 30],\n",
       " [30, 25, 27, 29],\n",
       " [29, 1, 1, 1],\n",
       " [1, 43, 45, 40],\n",
       " [40, 40, 39, 43],\n",
       " [43, 33, 38, 31],\n",
       " [31, 2, 73, 61],\n",
       " [61, 54, 73, 2],\n",
       " [2, 44, 71, 74],\n",
       " [74, 73, 61, 2]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(md.trn_ds)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200296"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.trn_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200296"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.trn_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(md.val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[67, 58, 72, 72]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(md.val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md.val_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the 3-Char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".cuda() means do the calculation on GPU.   \n",
    "This converts all parameters in the model from\n",
    "torch.Tensor to torch.cuda.Tensor.\n",
    "Inputs have to be Variable.cuda(), so both of these will cuda-ed the outputs.  \n",
    "Hence there will be no need to cuda the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Char3Model(vocab_size, n_fac).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check its methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.children of Char3Model(\n",
       "  (e): Embedding(85, 42)\n",
       "  (l_in): Linear(in_features=42, out_features=256, bias=True)\n",
       "  (l_hidden): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (l_out): Linear(in_features=256, out_features=85, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Char3Model.forward of Char3Model(\n",
       "  (e): Embedding(85, 42)\n",
       "  (l_in): Linear(in_features=42, out_features=256, bias=True)\n",
       "  (l_hidden): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (l_out): Linear(in_features=256, out_features=85, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=42, out_features=256, bias=True)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.l_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=256, out_features=256, bias=True)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.l_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader will pass one batch of size 512 at a time to model m to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xs,yt = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`md` will automatically pass `*V(xs)` to `m` in `fit`.  \n",
    "\n",
    "Let's use the untrained model like a layer, just to produce the outputs.  \n",
    "This will be batch size X vocab size = 512 X 85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-4.8706 -4.2780 -4.7269  ...  -4.6993 -4.3924 -4.2887\n",
       "-4.7283 -4.3229 -4.7110  ...  -4.7150 -4.3299 -4.3579\n",
       "-4.5632 -4.3377 -4.7691  ...  -4.3538 -4.5652 -4.2522\n",
       "          ...             ⋱             ...          \n",
       "-4.5965 -3.9759 -4.7243  ...  -4.5285 -4.6732 -4.1388\n",
       "-4.4654 -4.3620 -4.5897  ...  -4.4403 -4.7126 -4.1935\n",
       "-4.8598 -4.3971 -4.8340  ...  -4.4494 -4.4382 -4.4886\n",
       "[torch.cuda.FloatTensor of size 512x85 (GPU 0)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(*V(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Adam optimizer, specify the parameters to optimize, and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(m.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F.nll_loss is negative log likelihood loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb26b0bfdc6d40ffbc4f9d5ff9320992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      2.075      0.689412  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.68941])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 0.001) # why are we using a layer optimizer if we are not using learner?\n",
    "                    # there won't be SGDR or differential lerning rate to begin with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63473a149af440784cc0a0567bd66b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.813917   0.44118   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.44118])]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "?V\n",
    "Signature: V(x, requires_grad=False, volatile=False)\n",
    "Source:   \n",
    "def V(x, requires_grad=False, volatile=False):\n",
    "    '''creates a single or a list of pytorch tensors, depending on input x. '''\n",
    "    return map_over(x, lambda o: V_(o, requires_grad, volatile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "?VV\n",
    "def VV(x):\n",
    "    '''creates a single or a list of pytorch tensors, depending on input x. '''\n",
    "    return map_over(x, VV_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "??VV_\n",
    "\n",
    "def VV_(x): \n",
    "    '''creates a volatile tensor, which does not require gradients. '''\n",
    "    return create_variable(x, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "??create_variable\n",
    " \n",
    "def create_variable(x, volatile, requires_grad=False):\n",
    "    if type (x) != Variable:\n",
    "        if IS_TORCH_04: x = Variable(T(x), requires_grad=requires_grad)\n",
    "        else:           x = Variable(T(x), requires_grad=requires_grad, volatile=volatile)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look inside get_next() by printing every line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    print(\"1) Inputs inp: \", inp, \"\\n\")\n",
    "    idxs = T(np.array([char_indices[c] for c in inp]))\n",
    "    print(\"2) Convert to index idxs:\", idxs, \"\\n\")\n",
    "    p = m(*VV(idxs))\n",
    "    print(\"3) Calculate probability for dictionary p:\", p, \"\\n\")\n",
    "    i = np.argmax(to_np(p))\n",
    "    print(\"4) Get prediction i:\",i, \"\\n\")\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Inputs inp:  y.  \n",
      "\n",
      "2) Convert to index idxs: \n",
      " 78\n",
      " 10\n",
      "  2\n",
      "[torch.cuda.LongTensor of size 3 (GPU 0)]\n",
      " \n",
      "\n",
      "3) Calculate probability for dictionary p: Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-4.6038 -4.4689 -4.3727 -4.5109 -4.4358 -4.4557 -4.3810 -4.5054 -4.4593 -4.4198\n",
      "\n",
      "Columns 10 to 19 \n",
      "-4.4614 -4.3225 -4.5428 -4.6532 -4.3256 -4.4126 -4.5740 -4.2827 -4.5083 -4.5611\n",
      "\n",
      "Columns 20 to 29 \n",
      "-4.6086 -4.6325 -4.6438 -4.5238 -4.3041 -4.3279 -4.3085 -4.2459 -4.4396 -4.6418\n",
      "\n",
      "Columns 30 to 39 \n",
      "-4.4156 -4.2547 -4.3981 -4.7036 -4.2437 -4.3488 -4.3553 -4.4531 -4.4709 -4.3814\n",
      "\n",
      "Columns 40 to 49 \n",
      "-4.4526 -4.5411 -4.4912 -4.2298 -4.5139 -4.1931 -4.3423 -4.5070 -4.5447 -4.3264\n",
      "\n",
      "Columns 50 to 59 \n",
      "-4.3112 -4.5760 -4.6654 -4.3588 -4.4506 -4.4406 -4.5779 -4.2468 -4.4907 -4.4721\n",
      "\n",
      "Columns 60 to 69 \n",
      "-4.3431 -4.4567 -4.4477 -4.5343 -4.4035 -4.5879 -4.3351 -4.3739 -4.5189 -4.4786\n",
      "\n",
      "Columns 70 to 79 \n",
      "-4.6306 -4.6601 -4.4965 -4.4755 -4.4075 -4.4231 -4.4501 -4.3686 -4.3907 -4.5869\n",
      "\n",
      "Columns 80 to 84 \n",
      "-4.3304 -4.3737 -4.4400 -4.4853 -4.4892\n",
      "[torch.cuda.FloatTensor of size 1x85 (GPU 0)]\n",
      " \n",
      "\n",
      "4) Get prediction i: 45 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'U'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('y. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char_indices[c] for c in inp]))\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('ppl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(' th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('and')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ??? \n",
    "why does tranpose and not-tranpose produce the same answer???\n",
    "\n",
    "> idxs = T(np.array([char_indices[c] for c in inp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = (np.array([char_indices[c] for c in inp]))\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('ppl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(' th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('and')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first RNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the size of our unrolled RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence length = cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cs=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 42, 29, 30, 25, 27, 29, 1, 1, 1, 43, 45, 40, 40, 39, 43, 33, 38, 31, 2]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For each of 0 through 7, create a list of every 8th character with that starting point. These will be the 8 inputs to our model.  \n",
    "Note that **c_in_dat** is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[40, 42, 29, 30, 25, 27, 29, 1],\n",
       " [42, 29, 30, 25, 27, 29, 1, 1],\n",
       " [29, 30, 25, 27, 29, 1, 1, 1],\n",
       " [30, 25, 27, 29, 1, 1, 1, 43],\n",
       " [25, 27, 29, 1, 1, 1, 43, 45],\n",
       " [27, 29, 1, 1, 1, 43, 45, 40],\n",
       " [29, 1, 1, 1, 43, 45, 40, 40],\n",
       " [1, 1, 1, 43, 45, 40, 40, 39],\n",
       " [1, 1, 43, 45, 40, 40, 39, 43],\n",
       " [1, 43, 45, 40, 40, 39, 43, 33]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(len(idx)-cs)] # Note in lecture it is range(len(idx)-cs-1)\n",
    "c_in_dat[:10]                                                          # i assume lecture is wrong  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then create a list of the next character in each of these series. This will be the labels for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 43, 45, 40, 40, 39, 43, 33, 38, 31, 2, 73, 61, 54, 73, 2, 44, 71, 74]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_out_dat = [idx[j+cs] for j in range(len(idx)-cs)]\n",
    "c_out_dat[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xs = np.stack(c_in_dat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600885, 8)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y = np.stack(c_out_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So each column below is one series of 8 characters from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 42, 29, 30, 25, 27, 29,  1],\n",
       "       [42, 29, 30, 25, 27, 29,  1,  1],\n",
       "       [29, 30, 25, 27, 29,  1,  1,  1],\n",
       "       [30, 25, 27, 29,  1,  1,  1, 43],\n",
       "       [25, 27, 29,  1,  1,  1, 43, 45],\n",
       "       [27, 29,  1,  1,  1, 43, 45, 40],\n",
       "       [29,  1,  1,  1, 43, 45, 40, 40],\n",
       "       [ 1,  1,  1, 43, 45, 40, 40, 39]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[:cs,:cs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "...and this is the next character after each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1, 43, 45, 40, 40, 39, 43])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:cs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600884"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)-cs-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([480310, 419017, 232803, ..., 134355, 389158, 330599])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cv_idxs(len(idx)-cs-1)  # leave space for 8 chars of input + one char of outputs == cs+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ???\n",
    "Seems like there is no need to minus 1 in len(idx)-cs-1.  \n",
    "\"len(idx)-cs\" produces the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = get_cv_idxs(len(idx)-cs-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120176"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_arrays('.', val_idx, xs, y, bs=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses a sequential loop instead of building each layer independantly.  \n",
    "The hidden state variables and character variables are added together\n",
    "> h = F.tanh(self.l_hidden(h+inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLoopModel(nn.Module):\n",
    "    # This is an RNN!\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.l_in = nn.Linear(n_fac, n_hidden)\n",
    "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(bs, n_hidden).cuda()) \n",
    "        for c in cs:\n",
    "            inp = F.relu(self.l_in(self.e(c)))\n",
    "            h = F.tanh(self.l_hidden(h+inp))\n",
    "        \n",
    "        return F.log_softmax(self.l_out(h), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharLoopModel(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c8b5ce28c34d0b96442eadc2bc6d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      2.039299   2.012571  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2.01257])]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3ed0ecc8e647bebcacaa021ca6139a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.748343   1.74765   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.74765])]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char_indices[c] for c in inp])) # again, not tranposing gives the same result too\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('part of ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model (Concat, not add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model concatenate, not add, the hidden states variables and the character inputs.\n",
    "> self.l_in = nn.Linear(n_fac+n_hidden, n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLoopConcatModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.l_in = nn.Linear(n_fac+n_hidden, n_hidden)\n",
    "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(bs, n_hidden).cuda())\n",
    "        for c in cs:\n",
    "            inp = torch.cat((h, self.e(c)), 1)\n",
    "            inp = F.relu(self.l_in(inp))\n",
    "            h = F.tanh(self.l_hidden(inp))\n",
    "        \n",
    "        return F.log_softmax(self.l_out(h), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharLoopConcatModel(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c884ba3eac7469c801bb43f0a44efbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.86289    1.85946   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.85946])]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516a8fbcdcb34aacb837a80ff99a6b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.694402   1.69904   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.69904])]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char_indices[c] for c in inp]))\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('part of ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RNN with pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?nn.RNN\n",
    "\n",
    "Init signature: nn.RNN(*args, **kwargs)\n",
    "Docstring:     \n",
    "Applies a multi-layer Elman RNN with tanh or ReLU non-linearity to an\n",
    "input sequence.\n",
    "\n",
    "\n",
    "For each element in the input sequence, each layer computes the following\n",
    "function:\n",
    "\n",
    ".. math::\n",
    "\n",
    "    h_t = \\tanh(w_{ih} * x_t + b_{ih}  +  w_{hh} * h_{(t-1)} + b_{hh})\n",
    "\n",
    "where :math:`h_t` is the hidden state at time `t`, and :math:`x_t` is\n",
    "the hidden state of the previous layer at time `t` or :math:`input_t`\n",
    "for the first layer. If nonlinearity='relu', then `ReLU` is used instead\n",
    "of `tanh`.\n",
    "\n",
    "Args:\n",
    "    input_size: The number of expected features in the input x\n",
    "    hidden_size: The number of features in the hidden state h\n",
    "    num_layers: Number of recurrent layers.\n",
    "    nonlinearity: The non-linearity to use ['tanh'|'relu']. Default: 'tanh'\n",
    "    bias: If ``False``, then the layer does not use bias weights b_ih and b_hh.\n",
    "        Default: ``True``\n",
    "    batch_first: If ``True``, then the input and output tensors are provided\n",
    "        as (batch, seq, feature)\n",
    "    dropout: If non-zero, introduces a dropout layer on the outputs of each\n",
    "        RNN layer except the last layer\n",
    "    bidirectional: If ``True``, becomes a bidirectional RNN. Default: ``False``\n",
    "\n",
    "Inputs: input, h_0\n",
    "    - **input** (seq_len, batch, input_size): tensor containing the features\n",
    "      of the input sequence. The input can also be a packed variable length\n",
    "      sequence. See :func:`torch.nn.utils.rnn.pack_padded_sequence`\n",
    "      for details.\n",
    "    - **h_0** (num_layers * num_directions, batch, hidden_size): tensor\n",
    "      containing the initial hidden state for each element in the batch.\n",
    "      Defaults to zero if not provided.\n",
    "\n",
    "Outputs: output, h_n\n",
    "    - **output** (seq_len, batch, hidden_size * num_directions): tensor\n",
    "      containing the output features (h_k) from the last layer of the RNN,\n",
    "      for each k.  If a :class:`torch.nn.utils.rnn.PackedSequence` has\n",
    "      been given as the input, the output will also be a packed sequence.\n",
    "    - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n",
    "      containing the hidden state for k=seq_len.\n",
    "\n",
    "Attributes:\n",
    "    weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,\n",
    "        of shape `(hidden_size x input_size)` for k=0. Otherwise, the shape is\n",
    "        `(hidden_size x hidden_size)`\n",
    "    weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,\n",
    "        of shape `(hidden_size x hidden_size)`\n",
    "    bias_ih_l[k]: the learnable input-hidden bias of the k-th layer,\n",
    "        of shape `(hidden_size)`\n",
    "    bias_hh_l[k]: the learnable hidden-hidden bias of the k-th layer,\n",
    "        of shape `(hidden_size)`\n",
    "\n",
    "Examples::\n",
    "\n",
    "    >>> rnn = nn.RNN(10, 20, 2) # 2 layers\n",
    "    >>> input = Variable(torch.randn(5, 3, 10))  # (sequence length, bs, embedding)\n",
    "    >>> h0 = Variable(torch.randn(2, 3, 20)) # normally 1st argument is 1  (layer, bs, hidden)\n",
    "    >>> output, hn = rnn(input, h0)\n",
    "File:           ~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py\n",
    "Type:           type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Note that the 3rd arugument **num_layers** = `2`   \n",
    "nn.RNN(10, 20, `2`)  \n",
    "Variable(torch.randn(`2`, 3, 20))  \n",
    "\n",
    "This involves more complex archtecture of stacking 2 RNNs, one on top of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args:\n",
    "    input_size: The number of expected features in the input x\n",
    "    hidden_size: The number of features in the hidden state h\n",
    "    num_layers: Number of recurrent layers.\n",
    "        \n",
    "rnn = nn.RNN(10, 20, 2)\n",
    "h0 = Variable(torch.randn(2, 3, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if in the following code you only passed in `inp = self.e(cs)` you will have the following error later  \n",
    "when you run fit(): **tuple' object has no attribute 'dim**  \n",
    "  \n",
    "Hence you will have to stack the inputs again using `inp = self.e(torch.stack(cs))`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that nn.RNN() accept input of shape **(sequence length, bs, embedding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(1, bs, n_hidden))\n",
    "        inp = self.e(torch.stack(cs)) # See below regarding (sequence length, bs, embedding)\n",
    "        outp,h = self.rnn(inp, h)\n",
    "                \n",
    "        return F.log_softmax(self.l_out(outp[-1]), dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs shape to nn.RNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the input to `nn.RNN()` has to be of size (sequence length, bs, embedding). \n",
    "So when we pass in the inputs into `forward()`, it still has the size of (bs, 8). \n",
    "  \n",
    "However, using the splatter operator asterix in forward(), \n",
    "we split the inputs into eight separate vectors of size (bs) this way : \n",
    "`[(bs),(bs),(bs),(bs),(bs),(bs),(bs),(bs)]`\n",
    "\n",
    "calling torch.stack(cs) would be effectively stacking these vector on top of each other, \n",
    "in so creating a tensor of size(sequence, bs).\n",
    "\n",
    "Passing through the nn.embedding() makes it (sequence length, bs embedding)   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch RNNs return a tuple of **(output, h_n)**.    \n",
    "  \n",
    "Output contains the hidden state of the last RNN layer at the **last timestep**. This is usually what you want to pass downstream for sequence prediction tasks.    \n",
    "  \n",
    "h_n is the hidden state for t=seq_len (for all RNN layers and directions).  \n",
    "  \n",
    "output is a tensor of shape seq_len, batch_size, hidden_size * num_directions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So outputs are the final layer hidden state for all timestep. outp[-1] is the one for the last timestep. [link2](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwjQ_sut4ZTeAhWLvo8KHQ6-D-AQFjAAegQICRAB&url=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F48302810%2Fwhats-the-difference-between-hidden-and-output-in-pytorch-lstm&usg=AOvVaw3XUEYr6wUxuL0q3ANVhlUn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### self.rnn(inp, h)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no need to add a loop when using \n",
    "> self.rnn = nn.RNN(n_fac, n_hidden)  \n",
    "  \n",
    "as pytorch has abstracted it for us. These lines in the previous RNN as follows    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = V(torch.zeros(bs, n_hidden).cuda())  \n",
    "for c in cs:  \n",
    "    inp = torch.cat((h, self.e(c)), 1)  \n",
    "    inp = F.relu(self.l_in(inp))  \n",
    "    h = F.tanh(self.l_hidden(inp))  \n",
    "\n",
    "F.log_softmax(self.l_out(h), dim=-1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "are abstracted and become  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = V(torch.zeros(1, bs, n_hidden))\n",
    "inp = self.e(torch.stack(cs)) #(sequence length, bs, embedding)\n",
    "outp,h = self.rnn(inp, h)     \n",
    "\n",
    "F.log_softmax(self.l_out(outp[-1]), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the zero matrix hidden weights h has to be a rank 3 tensor **(1, bs, n_hidden)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharRnn(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pass a single batch of data into the rnn layer to see it in action.  \n",
    "To do so, first use the iter() on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xs,yt = next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the input dimension is   \n",
    "*Sequence length, Batch size, Embedding size* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the inputs are stacked to change the list to a torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   73    68    54  ...      2    57    58\n",
       "   62    56    62  ...     44    72    72\n",
       "   56    73    67  ...     32     2     2\n",
       "       ...          ⋱          ...       \n",
       "   62    67     2  ...      2     2     2\n",
       "   67    58    54  ...     73    73    70\n",
       "    2    10    67  ...     61    61    74\n",
       "[torch.cuda.LongTensor of size 8x512 (GPU 0)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length, Batch size, Embedding size : torch.Size([8, 512, 42])\n"
     ]
    }
   ],
   "source": [
    "input1 = m.e(V(torch.stack(xs)))\n",
    "print(\"Sequence length, Batch size, Embedding size :\",input1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the initial size of the hidden state variables are also similiar      \n",
    "*Number of Stacked RNN Layer, Batch size, Hidden state size* \n",
    "  \n",
    "Number of Stacked RNN Layer usually = 1 (unless complex architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Stacked RNN Layer, Batch size, Hidden state size : torch.Size([1, 512, 256])\n"
     ]
    }
   ],
   "source": [
    "ht = V(torch.zeros(1, 512,n_hidden))\n",
    "print(\"Number of Stacked RNN Layer, Batch size, Hidden state size :\",ht.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass a single batch into `m.rnn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp, hn = m.rnn(input1, ht)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running through the sequence length, the final hidden state size is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 256])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the output has a size of 8 (axis 0), equal to the sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 256])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare to forward pass the batches of 512 rows of inputs into the model.  \n",
    "For every row of input, it will produce a prediction for each of the 85 characters.  \n",
    "Note that the model m can be used like a layer, just like what was done with self.rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 85])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = m(*V(xs)); t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544441f28e6e49ebbf6cd47a5c46cfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.871748   1.841063  \n",
      "    1      1.672987   1.675138                              \n",
      "    2      1.582195   1.602557                              \n",
      "    3      1.542989   1.555944                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.55594])]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf7de4848034aafa94811046dfc204a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.472884   1.511173  \n",
      "    1      1.46085    1.505596                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.5056])]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 2, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char_indices[c] for c in inp])) # why transpose? \n",
    "                                                       # see Inputs shape to nn.RNN()\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for those something the same to the same to the '"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n('for thos', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked RNN ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try building a 2-layer RNN ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  self.rnn = nn.RNN(n_fac, n_hidden,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharStackedRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden,2)   # changed to 2\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(2, bs, n_hidden)) # changed to 2\n",
    "        inp = self.e(torch.stack(cs))\n",
    "        outp,h = self.rnn(inp, h)\n",
    "        \n",
    "        return F.log_softmax(self.l_out(outp[-1]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharStackedRnn(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xs,yt = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length, Batch size, Embedding size : torch.Size([8, 512, 42])\n"
     ]
    }
   ],
   "source": [
    "input1 = m.e(V(torch.stack(xs)))\n",
    "print(\"Sequence length, Batch size, Embedding size :\",input1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Stacked RNN Layer, Batch size, Hidden state size : torch.Size([2, 512, 256])\n"
     ]
    }
   ],
   "source": [
    "ht = V(torch.zeros(2, 512,n_hidden))\n",
    "print(\"Number of Stacked RNN Layer, Batch size, Hidden state size :\",ht.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp, hn = m.rnn(input1, ht)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running through the sequence length, the final hidden state size is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 256])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas the output has a size of 8 (axis 0), equal to the sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 256])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 85])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = m(*V(xs)); t.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfits quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7366b3d0d514b92b000ed2f9c92cf91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.733756   1.723142  \n",
      "    1      1.56492    1.569838                              \n",
      "    2      1.488353   1.507365                              \n",
      "    3      1.445725   1.47659                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.47659])]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char_indices[c] for c in inp]))\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for those of the same something the same somethi'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_n('for thos', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multi-output model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's take non-overlapping sets of characters this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[40, 42, 29, 30, 25, 27, 29, 1],\n",
       " [1, 1, 43, 45, 40, 40, 39, 43],\n",
       " [33, 38, 31, 2, 73, 61, 54, 73],\n",
       " [2, 44, 71, 74, 73, 61, 2, 62],\n",
       " [72, 2, 54, 2, 76, 68, 66, 54],\n",
       " [67, 9, 9, 76, 61, 54, 73, 2],\n",
       " [73, 61, 58, 67, 24, 2, 33, 72],\n",
       " [2, 73, 61, 58, 71, 58, 2, 67],\n",
       " [68, 73, 2, 60, 71, 68, 74, 67],\n",
       " [57, 1, 59, 68, 71, 2, 72, 74]]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(0, len(idx)-cs-1, cs)]\n",
    "c_in_dat[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then create the exact same thing, offset by 1, as our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[42, 29, 30, 25, 27, 29, 1, 1],\n",
       " [1, 43, 45, 40, 40, 39, 43, 33],\n",
       " [38, 31, 2, 73, 61, 54, 73, 2],\n",
       " [44, 71, 74, 73, 61, 2, 62, 72],\n",
       " [2, 54, 2, 76, 68, 66, 54, 67],\n",
       " [9, 9, 76, 61, 54, 73, 2, 73],\n",
       " [61, 58, 67, 24, 2, 33, 72, 2],\n",
       " [73, 61, 58, 71, 58, 2, 67, 68],\n",
       " [73, 2, 60, 71, 68, 74, 67, 57],\n",
       " [1, 59, 68, 71, 2, 72, 74, 72]]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_out_dat = [[idx[i+j] for i in range(cs)] for j in range(1, len(idx)-cs, cs)]\n",
    "c_out_dat[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minor technical note: that for the above, inputs range is `range(0, len(idx)-cs-1`. There is a minus 1.     \n",
    "but output range starts from 1 and is `range(1, len(idx)-cs)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75111, 8)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = np.stack(c_in_dat)\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75111, 8)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = np.stack(c_out_dat)\n",
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40, 42, 29, 30, 25, 27, 29,  1],\n",
       "       [ 1,  1, 43, 45, 40, 40, 39, 43],\n",
       "       [33, 38, 31,  2, 73, 61, 54, 73],\n",
       "       [ 2, 44, 71, 74, 73, 61,  2, 62],\n",
       "       [72,  2, 54,  2, 76, 68, 66, 54],\n",
       "       [67,  9,  9, 76, 61, 54, 73,  2],\n",
       "       [73, 61, 58, 67, 24,  2, 33, 72],\n",
       "       [ 2, 73, 61, 58, 71, 58,  2, 67]])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[:cs,:cs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42, 29, 30, 25, 27, 29,  1,  1],\n",
       "       [ 1, 43, 45, 40, 40, 39, 43, 33],\n",
       "       [38, 31,  2, 73, 61, 54, 73,  2],\n",
       "       [44, 71, 74, 73, 61,  2, 62, 72],\n",
       "       [ 2, 54,  2, 76, 68, 66, 54, 67],\n",
       "       [ 9,  9, 76, 61, 54, 73,  2, 73],\n",
       "       [61, 58, 67, 24,  2, 33, 72,  2],\n",
       "       [73, 61, 58, 71, 58,  2, 67, 68]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys[:cs,:cs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get validation index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75102"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xs)-cs-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 20% will be used as validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15020"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_idx = get_cv_idxs(len(xs)-cs-1)\n",
    "val_idx.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_arrays('.', val_idx, xs, ys, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(1, bs, n_hidden))\n",
    "        inp = self.e(torch.stack(cs))\n",
    "        outp,h = self.rnn(inp, h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This *multi-output RNN* has exactly the same codes as *RNN with pytorch* except for the following: \n",
    "\n",
    "RNN with pytorch returns one output  \n",
    "**return F.log_softmax(self.l_out(outp[-1]), dim=-1)**  \n",
    "  \n",
    "multi-output RNN returns all outputs  \n",
    "**return F.log_softmax(self.l_out(outp), dim=-1)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqRnn(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xst,yt = next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target `yt` will need to be transposed and flattened for the **convenience** of the loss function as well as the shape of the output from nn.RNN()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tranpose target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that yt the outer dim is 512 and the inner dim is 8. so even when you transpose, the inner dim remains as 8 even though yt has new shape (8,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 8])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yt.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytt = yt.transpose(0,1)\n",
    "ytt.size() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it has to be flattened from 8 X 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    8    73    58  ...     62    72    47\n",
       "    2    61    71  ...     73    68    54\n",
       "   62    58    75  ...     61    69    60\n",
       "       ...          ⋱          ...       \n",
       "   54    74    73  ...     32     2    71\n",
       "   57    67     4  ...     29    62     5\n",
       "   57    58     9  ...     37    67    72\n",
       "[torch.cuda.LongTensor of size 8x512 (GPU 0)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to a column tensor of 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  8\n",
       " 73\n",
       " 58\n",
       " ⋮ \n",
       " 37\n",
       " 67\n",
       " 72\n",
       "[torch.cuda.LongTensor of size 4096 (GPU 0)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytt.contiguous().view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flattenning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.reshape(list(range(60)),(3,4,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       "   0   1   2   3   4\n",
       "   5   6   7   8   9\n",
       "  10  11  12  13  14\n",
       "  15  16  17  18  19\n",
       "\n",
       "(1 ,.,.) = \n",
       "  20  21  22  23  24\n",
       "  25  26  27  28  29\n",
       "  30  31  32  33  34\n",
       "  35  36  37  38  39\n",
       "\n",
       "(2 ,.,.) = \n",
       "  40  41  42  43  44\n",
       "  45  46  47  48  49\n",
       "  50  51  52  53  54\n",
       "  55  56  57  58  59\n",
       "[torch.FloatTensor of size 3x4x5]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     1     2     3     4\n",
       "    5     6     7     8     9\n",
       "   10    11    12    13    14\n",
       "   15    16    17    18    19\n",
       "   20    21    22    23    24\n",
       "   25    26    27    28    29\n",
       "   30    31    32    33    34\n",
       "   35    36    37    38    39\n",
       "   40    41    42    43    44\n",
       "   45    46    47    48    49\n",
       "   50    51    52    53    54\n",
       "   55    56    57    58    59\n",
       "[torch.FloatTensor of size 12x5]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(-1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 12 \n",
       "    0     1     2     3     4     5     6     7     8     9    10    11    12\n",
       "   20    21    22    23    24    25    26    27    28    29    30    31    32\n",
       "   40    41    42    43    44    45    46    47    48    49    50    51    52\n",
       "\n",
       "Columns 13 to 19 \n",
       "   13    14    15    16    17    18    19\n",
       "   33    34    35    36    37    38    39\n",
       "   53    54    55    56    57    58    59\n",
       "[torch.FloatTensor of size 3x20]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 12 \n",
       "    0     1     2     3     4     5     6     7     8     9    10    11    12\n",
       "   15    16    17    18    19    20    21    22    23    24    25    26    27\n",
       "   30    31    32    33    34    35    36    37    38    39    40    41    42\n",
       "   45    46    47    48    49    50    51    52    53    54    55    56    57\n",
       "\n",
       "Columns 13 to 14 \n",
       "   13    14\n",
       "   28    29\n",
       "   43    44\n",
       "   58    59\n",
       "[torch.FloatTensor of size 4x15]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(4,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     1     2     3\n",
       "    4     5     6     7\n",
       "    8     9    10    11\n",
       "   12    13    14    15\n",
       "   16    17    18    19\n",
       "   20    21    22    23\n",
       "   24    25    26    27\n",
       "   28    29    30    31\n",
       "   32    33    34    35\n",
       "   36    37    38    39\n",
       "   40    41    42    43\n",
       "   44    45    46    47\n",
       "   48    49    50    51\n",
       "   52    53    54    55\n",
       "   56    57    58    59\n",
       "[torch.FloatTensor of size 15x4]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(-1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the inputs, which are the predictions, and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 85])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = m(*V(xst))\n",
    "pred.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feed pred and target into the loss function nll_loss_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]],\n",
       "\n",
       "       [[12, 13, 14, 15],\n",
       "        [16, 17, 18, 19],\n",
       "        [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(24).reshape((2,3,4))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [12, 13, 14, 15],\n",
       "       [16, 17, 18, 19],\n",
       "       [20, 21, 22, 23]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(-1,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note tha pytorch does not accept rank3 tensor. As such, the inputs have to be flatten to **inputs.view(-1,nh)**  \n",
    "The last axis is kept while the rest are flattened into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,_,nh = pred.size()\n",
    "nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8X512 = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-4.3087 -4.3666 -4.6273  ...  -4.6672 -4.3569 -4.6242\n",
       "-4.4377 -4.4526 -4.3745  ...  -4.4565 -4.3258 -4.4461\n",
       "-4.4238 -4.8020 -4.2584  ...  -4.5244 -4.2242 -4.4699\n",
       "          ...             ⋱             ...          \n",
       "-4.4313 -4.7284 -4.3200  ...  -4.5356 -4.3340 -4.2562\n",
       "-4.3487 -4.4368 -4.7077  ...  -4.7442 -4.1969 -4.5650\n",
       "-4.1541 -4.2702 -4.7339  ...  -4.6129 -4.1995 -4.4297\n",
       "[torch.cuda.FloatTensor of size 4096x85 (GPU 0)]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.view(-1,nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(inp, targ):\n",
    "    sl,bs,nh = inp.size()\n",
    "    print(\"sl,bs,nh: \", sl,bs,nh)\n",
    "    print(\"\\n\")\n",
    "    targ = targ.transpose(0,1).contiguous().view(-1) # (sequence length, bs)\n",
    "    print(\"targ:\", targ)\n",
    "    print(\"\\n\")\n",
    "    print(\"loss function output:\")\n",
    "    print(\"\\n\")\n",
    "    print(\"'inp.view(-1,nh):' is inp flattened so that it can be fed into the loss function \")\n",
    "    print(inp.view(-1,nh)) # #(sequence length, bs, embedding)\n",
    "    return F.nll_loss(inp.view(-1,nh), targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `F.nll_loss` is the **negative log likelihood loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the function and print out all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sl,bs,nh:  8 512 85\n",
      "\n",
      "\n",
      "targ: Variable containing:\n",
      " 58\n",
      " 21\n",
      " 54\n",
      " ⋮ \n",
      " 74\n",
      " 68\n",
      " 57\n",
      "[torch.cuda.LongTensor of size 4096 (GPU 0)]\n",
      "\n",
      "\n",
      "\n",
      "loss function output:\n",
      "\n",
      "\n",
      "'inp.view(-1,nh):' is inp flattened so that it can be fed into the loss function \n",
      "Variable containing:\n",
      "-4.3087 -4.3666 -4.6273  ...  -4.6672 -4.3569 -4.6242\n",
      "-4.4377 -4.4526 -4.3745  ...  -4.4565 -4.3258 -4.4461\n",
      "-4.4238 -4.8020 -4.2584  ...  -4.5244 -4.2242 -4.4699\n",
      "          ...             ⋱             ...          \n",
      "-4.4313 -4.7284 -4.3200  ...  -4.5356 -4.3340 -4.2562\n",
      "-4.3487 -4.4368 -4.7077  ...  -4.7442 -4.1969 -4.5650\n",
      "-4.1541 -4.2702 -4.7339  ...  -4.6129 -4.1995 -4.4297\n",
      "[torch.cuda.FloatTensor of size 4096x85 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 4.4668\n",
       "[torch.cuda.FloatTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll_loss_seq(pred, V(yt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(inp, targ):\n",
    "    sl,bs,nh = inp.size() # shouldn't this be sl,bs,vocab = inp.size() instead?\n",
    "    targ = targ.transpose(0,1).contiguous().view(-1)\n",
    "    # targ is (512,8) but inp is (8,512,84), so transpose, then flatten using view(-1)\n",
    "    \n",
    "    return F.nll_loss(inp.view(-1,nh), targ)\n",
    "    # flatten inp's first 2 axis (8,512,84) so that we can compare 84 probabilities with each targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1867cc39ff7476e9dfe851d145054a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      2.571819   2.395257  \n",
      "    1      2.278041   2.189023                              \n",
      "    2      2.126606   2.075855                              \n",
      "    3      2.0344     2.001645                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2.00164])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 4, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e51ad62bfd42a1960d203d5f8a787e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.986922   1.987179  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.98718])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Identity init!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we test the model by using the Identity Matrix instead of a random initializer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m = CharSeqRnn(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 module layers in the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of CharSeqRnn(\n",
       "  (e): Embedding(85, 42)\n",
       "  (rnn): RNN(42, 256)\n",
       "  (l_out): Linear(in_features=256, out_features=85, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The character input weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 42])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.rnn.weight_ih_l0.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the hidden state weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.rnn.weight_hh_l0.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these weights are randomly initialized, which is not ideal. The hidden weights are to be passed from one sequence to the next, so they are likely to explode or vanish. Using the Identity matrix can help to keep them stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       " 2.9081e-02 -2.7065e-02  9.5262e-03  ...  -5.7022e-02  1.6494e-02 -1.8046e-02\n",
       "-6.0399e-02 -3.7136e-02 -3.4931e-02  ...  -5.9569e-02  1.1623e-02  4.1199e-02\n",
       "-2.0252e-02 -6.9852e-03 -5.7479e-02  ...   4.7481e-02  6.0246e-02 -4.6353e-02\n",
       "                ...                   ⋱                   ...                \n",
       "-1.2451e-02 -1.1864e-02 -4.8800e-02  ...   2.2607e-02 -4.8948e-02  2.6694e-02\n",
       " 2.2739e-02 -4.8428e-02 -2.4000e-02  ...   2.7301e-02 -4.3589e-02  4.0415e-02\n",
       " 3.1339e-02  3.8786e-02 -3.4345e-02  ...   5.0805e-02  4.7381e-02 -2.8259e-02\n",
       "[torch.cuda.FloatTensor of size 256x256 (GPU 0)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.rnn.weight_hh_l0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Identity Matrix instead. **Note that the identity matrix can only be used in a square matrix, so only the hidden layer can be initiated this way.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    1     0     0  ...      0     0     0\n",
       "    0     1     0  ...      0     0     0\n",
       "    0     0     1  ...      0     0     0\n",
       "       ...          ⋱          ...       \n",
       "    0     0     0  ...      1     0     0\n",
       "    0     0     0  ...      0     1     0\n",
       "    0     0     0  ...      0     0     1\n",
       "[torch.cuda.FloatTensor of size 256x256 (GPU 0)]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.rnn.weight_hh_l0.data.copy_(torch.eye(n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098644f89a854f049888278bbb4838b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      2.188063   2.037499  \n",
      "    1      1.973951   1.92179                               \n",
      "    2      1.895284   1.882794                             \n",
      "    3      1.850385   1.856922                              \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.85692])]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 4, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7663553c46d543e9bad97a5c39b312af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                              \n",
      "    0      1.746195   1.781623  \n",
      "    1      1.732748   1.774872                              \n",
      "    2      1.725642   1.77018                               \n",
      "    3      1.716911   1.76791                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.76791])]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 4, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a specific way that torch lines up text data for stateful rnn. Watch it [here](https://youtu.be/H3g26EVADgY?t=16m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mmodels\u001b[0m/  nietzsche.txt  \u001b[01;34mtrn\u001b[0m/  \u001b[01;34mval\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "from torchtext import vocab, data\n",
    "\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "PATH='data/nietzsche/'\n",
    "\n",
    "TRN_PATH = 'trn/'\n",
    "VAL_PATH = 'val/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "\n",
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls {PATH}trn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Function LanguageModelData "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Field class models common text processing datatypes that can be represented\n",
    "    by tensors.  It holds a Vocab object that defines the set of possible values\n",
    "    for elements of the field and their corresponding numerical representations. \n",
    "    \n",
    "The Field object also holds other parameters relating to how a datatype\n",
    "    should be numericalized, such as a tokenization method and the kind of\n",
    "    Tensor that should be produced. If a Field is shared between two columns in a dataset (e.g., question and answer in a QA dataset), then they will have a shared vocabulary.  \n",
    "      \n",
    "      \n",
    "Basically, it shows you how text is mapped to numbers, and what are the logic behind this mapping.      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tokenize =list` uses built-in function *list* as the function to tokenize the text strings.  \n",
    "If `tokenize = spacy` is selected, the SpaCy English tokenizer is used.  \n",
    "The **default** tokenize function is just `str.split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64; bptt=8; n_fac=42; n_hidden=256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILES is just shortcut indicating where the training,validation and test paths are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 'trn/', 'validation': 'val/', 'test': 'val/'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @classmethod\n",
    "    def from_text_files(cls, path, field, train, validation, test=None, bs=64, bptt=70, **kwargs):\n",
    "        \"\"\" Method used to instantiate a LanguageModelData object that can be used for a\n",
    "            supported nlp task.\n",
    "\n",
    "        Args:\n",
    "            path (str): the absolute path in which temporary model data will be saved\n",
    "            field (Field): torchtext field\n",
    "            train (str): file location of the training data\n",
    "            validation (str): file location of the validation data\n",
    "            test (str): file location of the testing data\n",
    "            bs (int): batch size to use\n",
    "            bptt (int): back propagation through time hyper-parameter\n",
    "            kwargs: other arguments\n",
    "\n",
    "        Returns:\n",
    "            a LanguageModelData instance, which most importantly, provides us the datasets for training,\n",
    "                validation, and testing\n",
    "\n",
    "        Note:\n",
    "            The train, validation, and test path can be pointed to any file (or folder) that contains a valid\n",
    "                text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 953\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of batches: {len(md.trn_dl)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the built-in function *list* is used in *tokenize=list*, giving the number of tokens as 55. \n",
    "\n",
    "*TEXT = data.Field(lower=True, tokenize=list)*  \n",
    "*LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)* \n",
    "\n",
    "Recall that in the beginning of this notebook we did not use the above to tokenize\n",
    "but instead did the tokenizing ourselves using\n",
    "\n",
    "*chars = sorted(list(set(text)))* \n",
    "  \n",
    "giving us 85 unique tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 55\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of tokens: {md.nt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out all the tokens. TEXT.vocab is created already by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk><pad> etiaonsrhldcufmpg,ywbv-.\"kx;:qj!?()\\'z12=_53[]468790ä'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(list(TEXT.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training dataset: 1\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training dataset: {len(md.trn_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one dataset because the whole chunk of Nietzche text is the dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in training dataset: 488745\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of tokens in training dataset: {len(md.trn_ds[0].text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the RNN  \n",
    "  \n",
    "At the end of an epoch, the batchsize will be lower than 64. To solve this wrinkle, forward() will check `if self.h.size(1) != bs: self.init_hidden(bs)` (Note that there will not be stateful in this case). This will set a new batch size for the final batch of the existing epoch as well as reset it to 64 when the new epoch starts.  \n",
    "\n",
    "`self.h = repackage_var(h)` will forget the prior operations so that the backprop will not go back in history all the way, this keeps the bptt to 8 time-steps.\n",
    "\n",
    "`F.log_softmax(self.l_out(outp), dim=-1)` will apply softmax on the last dimension (vocab) and then `.view(-1, self.vocab_size)` will flatten bs and bptt while keeping the last dimension vocab instact. Note that for the target, pytorch automatically flattens it so there is no need to do the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        self.vocab_size = vocab_size\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs): # note no splatter used (LanguageModelData)\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs) \n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ???Why is it that no splatter or torch.stack needs to be applied to cs when using LanguageDataModel???  \n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`repackage_var(h)` keeps the data but throws away all history of operations. This enables us to pass the state to the next minibatch, yet still have the backprop stop going back more than bptt timestep (in this case 8 timesteps). Otherwise as training goes on, self.h will carry longer and longer history, and backprop will go back in time earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the final output is flattened inside forward().  \n",
    "\n",
    "`F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regarding hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get stateful, the hidden state is declared now as an attribute as below. It was previously in def forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        self.init_hidden(bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final hidden states at the end of each mini-batch will be updated,  \n",
    "so that it will be used as the new initial hidden weights as the next mini-batch begins.  \n",
    "repackage_var() will forget the history of the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, cs):\n",
    "        outp,h = \n",
    "        self.h = repackage_var(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve the wrinkle of the last batch in an epoch having a different size than the other batches,  \n",
    "we have to use this condition in the forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, cs):\n",
    "        bs = cs[0].size(0) # batch size will be different for the very last batch in an epcoh\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ???\n",
    "\n",
    "Given the way torch split the corpus into batches to allow for stateful learning, how does torch   \n",
    "reconcile the change in batch size in the final batch of an epoch?  \n",
    "i.e. How does it line up the leftover text in the different batches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulRnn(md.nt, n_fac, 512).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5550c8b106934081ad41854b185ddc6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.872604   1.847967  \n",
      "    1      1.69718    1.70512                                \n",
      "    2      1.615404   1.635051                               \n",
      "    3      1.565641   1.603767                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.60377])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fdcac213ef4167ab741e349dea8425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.482862   1.552192  \n",
      "    1      1.487418   1.545811                               \n",
      "    2      1.473185   1.542076                               \n",
      "    3      1.478708   1.537967                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.53797])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_lrs(opt, 1e-4)\n",
    "\n",
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pedagogical exposition, let's use RNNCell() from pytorch instead.  \n",
    "This means that we will have to do the loop ourselves.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From the pytorch source\n",
    "# just to show that torch isn't doing anything special\n",
    "\n",
    "def RNNCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
    "    return F.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read comments added in the codes below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn2(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNNCell(n_fac, n_hidden) # Use custom RNN \n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
    "        outp = []  # need empty list to store output at each sequence\n",
    "        o = self.h  \n",
    "        for c in cs: # need to use for loop as not using nn.rnn()\n",
    "            o = self.rnn(self.e(c), o)\n",
    "            outp.append(o) # keep all hidden state in a list\n",
    "        outp = self.l_out(torch.stack(outp)) # get multiple outputs\n",
    "        self.h = repackage_var(o) # note that o is the final time step hidden state\n",
    "                                  # repackage_var so that self.h has no history\n",
    "                                  # repackage_var() == Varable(self.h.data)  \n",
    "        return F.log_softmax(outp, dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the above uses `LanguageModelData` to load data, while previously we were using `ColumnarModelData` to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using ColumnarModelData the splater and stack were used to change the shape of inp\n",
    "\n",
    "def forward(self, *cs): # Note *\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(1, bs, n_hidden))\n",
    "        inp = self.e(torch.stack(cs)) # have to stack to change shape to (seq,bs,embedding)\n",
    "        outp,h = self.rnn(inp, h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulRnn2(md.nt, n_fac, 512).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e964f8faeb4df58414ea98ebc421e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.873116   1.853935  \n",
      "    1      1.695284   1.70925                                \n",
      "    2      1.60732    1.645366                               \n",
      "    3      1.565484   1.604898                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.6049])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GRU class below is the same as *CharSeqStatefulRnn()*, except for one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.GRU(n_fac, n_hidden)  # Only this line is changed from nn.rnn()\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the pytorch source code - for reference\n",
    "\n",
    "def GRUCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
    "    gi = F.linear(input, w_ih, b_ih) # character\n",
    "    gh = F.linear(hidden, w_hh, b_hh) # hidden\n",
    "    i_r, i_i, i_n = gi.chunk(3, 1) # split(?) into the 3 respective gates\n",
    "    h_r, h_i, h_n = gh.chunk(3, 1) # same\n",
    "\n",
    "    resetgate = F.sigmoid(i_r + h_r)\n",
    "    inputgate = F.sigmoid(i_i + h_i)\n",
    "    newgate = F.tanh(i_n + resetgate * h_n) # new state\n",
    "    return newgate + inputgate * (hidden - newgate) \n",
    "          # hidden*(inputgate) + (1-inputgate)*newgate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suspect that `gi` and `gh` is actually 3 times bigger than usual, so that it can be split into 3 chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  0   1   2\n",
       "  3   4   5\n",
       "  6   7   8\n",
       "  9  10  11\n",
       "[torch.FloatTensor of size 4x3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.arange(12)\n",
    "v = v.view(4,3)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  0\n",
       "  3\n",
       "  6\n",
       "  9\n",
       " [torch.FloatTensor of size 4x1], \n",
       "   1\n",
       "   4\n",
       "   7\n",
       "  10\n",
       " [torch.FloatTensor of size 4x1], \n",
       "   2\n",
       "   5\n",
       "   8\n",
       "  11\n",
       " [torch.FloatTensor of size 4x1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.chunk(3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulGRU(md.nt, n_fac, 512).cuda()\n",
    "\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06d935963e04b9e8790e6d2394622cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.741781   1.726211  \n",
      "    1      1.563426   1.578207                               \n",
      "    2      1.476227   1.516808                               \n",
      "    3      1.427569   1.484527                               \n",
      "    4      1.384639   1.463643                               \n",
      "    5      1.35995    1.456252                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.45625])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 6, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba0f984817e44aea6fb7e782791113f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.2754     1.423165  \n",
      "    1      1.273323   1.418575                               \n",
      "    2      1.285239   1.417631                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.41763])]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 3, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import sgdr\n",
    "\n",
    "n_hidden=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "??nn.LSTM\n",
    "\n",
    "Init signature: nn.LSTM(*args, **kwargs)\n",
    "Source:        \n",
    "class LSTM(RNNBase):\n",
    "    r\"\"\"Applies a multi-layer long short-term memory (LSTM) RNN to an input\n",
    "    sequence.\n",
    "\n",
    "\n",
    "    For each element in the input sequence, each layer computes the following\n",
    "    function:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "            \\begin{array}{ll}\n",
    "            i_t = \\mathrm{sigmoid}(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
    "            f_t = \\mathrm{sigmoid}(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
    "            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hc} h_{(t-1)} + b_{hg}) \\\\\n",
    "            o_t = \\mathrm{sigmoid}(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
    "            c_t = f_t * c_{(t-1)} + i_t * g_t \\\\\n",
    "            h_t = o_t * \\tanh(c_t)\n",
    "            \\end{array}\n",
    "\n",
    "    where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\n",
    "    state at time `t`, :math:`x_t` is the hidden state of the previous layer at\n",
    "    time `t` or :math:`input_t` for the first layer, and :math:`i_t`,\n",
    "    :math:`f_t`, :math:`g_t`, :math:`o_t` are the input, forget, cell,\n",
    "    and out gates, respectively.\n",
    "\n",
    "    Args:\n",
    "        input_size: The number of expected features in the input x\n",
    "        hidden_size: The number of features in the hidden state h\n",
    "        num_layers: Number of recurrent layers.\n",
    "        bias: If ``False``, then the layer does not use bias weights b_ih and b_hh.\n",
    "            Default: ``True``\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature)\n",
    "        dropout: If non-zero, introduces a dropout layer on the outputs of each\n",
    "            RNN layer except the last layer\n",
    "        bidirectional: If ``True``, becomes a bidirectional RNN. Default: ``False``\n",
    "\n",
    "    Inputs: input, (h_0, c_0)\n",
    "        - **input** (seq_len, batch, input_size): tensor containing the features\n",
    "          of the input sequence.\n",
    "          The input can also be a packed variable length sequence.\n",
    "          See :func:`torch.nn.utils.rnn.pack_padded_sequence` for details.\n",
    "        - **h_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n",
    "          containing the initial hidden state for each element in the batch.\n",
    "        - **c_0** (num_layers \\* num_directions, batch, hidden_size): tensor\n",
    "          containing the initial cell state for each element in the batch.\n",
    "\n",
    "          If (h_0, c_0) is not provided, both **h_0** and **c_0** default to zero.\n",
    "\n",
    "\n",
    "    Outputs: output, (h_n, c_n)\n",
    "        - **output** (seq_len, batch, hidden_size * num_directions): tensor\n",
    "          containing the output features `(h_t)` from the last layer of the RNN,\n",
    "          for each t. If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n",
    "          given as the input, the output will also be a packed sequence.\n",
    "        - **h_n** (num_layers * num_directions, batch, hidden_size): tensor\n",
    "          containing the hidden state for t=seq_len\n",
    "        - **c_n** (num_layers * num_directions, batch, hidden_size): tensor\n",
    "          containing the cell state for t=seq_len\n",
    "\n",
    "    Attributes:\n",
    "        weight_ih_l[k] : the learnable input-hidden weights of the k-th layer\n",
    "            `(W_ii|W_if|W_ig|W_io)`, of shape `(4*hidden_size x input_size)`\n",
    "        weight_hh_l[k] : the learnable hidden-hidden weights of the k-th layer\n",
    "            `(W_hi|W_hf|W_hg|W_ho)`, of shape `(4*hidden_size x hidden_size)`\n",
    "        bias_ih_l[k] : the learnable input-hidden bias of the k-th layer\n",
    "            `(b_ii|b_if|b_ig|b_io)`, of shape `(4*hidden_size)`\n",
    "        bias_hh_l[k] : the learnable hidden-hidden bias of the k-th layer\n",
    "            `(b_hi|b_hf|b_hg|b_ho)`, of shape `(4*hidden_size)`\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> rnn = nn.LSTM(10, 20, 2)\n",
    "        >>> input = Variable(torch.randn(5, 3, 10))\n",
    "        >>> h0 = Variable(torch.randn(2, 3, 20))\n",
    "        >>> c0 = Variable(torch.randn(2, 3, 20))\n",
    "        >>> output, hn = rnn(input, (h0, c0))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size,self.nl = vocab_size,nl\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5) \n",
    "                                    # dropouts allow bs to increase to 512\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n",
    "                  V(torch.zeros(self.nl, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that LSTM requires 2 zero matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hidden(self, bs):\n",
    "        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n",
    "                  V(torch.zeros(self.nl, bs, n_hidden)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `LayerOptimizer` allows you to pass in a callback during fit(). If you just use `optim.Adam(m.parameters(), lr)` you will get the plain optimizer. To use SGDR, discrimintative learning rates and LR finders, you must use `LayerOptimzer()`, which will create a object that will allow for callbacks such as SGDR (CosAnnealing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulLSTM(md.nt, n_fac, 512, 2).cuda()\n",
    "lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{PATH}models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea3ec2f02394e7bb8300820e4ef50d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.795547   1.731401  \n",
      "    1      1.696436   1.645172                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.64517])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 2, lo.opt, F.nll_loss) # note that lo.opt is inserted as optimzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The callback to be used, CosAnneal, requires a layer optimizer object, which is *lo*  \n",
    "CosAnneal() will change the learning rate inside *lo* during the training process (???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosAnneal(LR_Updater): # == SGDR\n",
    "    ''' Learning rate scheduler that inpelements a cosine annealation schedule. '''\n",
    "    def __init__(self, layer_opt,                             ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Init signature: CosAnneal(layer_opt, nb, on_cycle_end=None, cycle_mult=1)\n",
    "Source:        \n",
    "class CosAnneal(LR_Updater):\n",
    "    ''' Learning rate scheduler that inpelements a cosine annealation schedule. '''\n",
    "    def __init__(self, layer_opt, nb, on_cycle_end=None, cycle_mult=1):\n",
    "        self.nb,self.on_cycle_end,self.cycle_mult = nb,on_cycle_end,cycle_mult\n",
    "        super().__init__(layer_opt)\n",
    "\n",
    "    def on_train_begin(self):\n",
    "        self.cycle_iter,self.cycle_count=0,0\n",
    "        super().on_train_begin()\n",
    "\n",
    "    def calc_lr(self, init_lrs):\n",
    "        if self.iteration<self.nb/20:\n",
    "            self.cycle_iter += 1\n",
    "            return init_lrs/100.\n",
    "\n",
    "        cos_out = np.cos(np.pi*(self.cycle_iter)/self.nb) + 1\n",
    "        self.cycle_iter += 1\n",
    "        if self.cycle_iter==self.nb:\n",
    "            self.cycle_iter = 0\n",
    "            self.nb *= self.cycle_mult\n",
    "            if self.on_cycle_end: self.on_cycle_end(self, self.cycle_count)\n",
    "            self.cycle_count += 1\n",
    "        return init_lrs / 2 * cos_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ???  \n",
    "note that if argument **sched** is left out, fit() will have a problem.  \n",
    "*on_end = lambda cycle: save_model(m, f'{PATH}models/cyc_{cycle}')*  \n",
    "but not clear of its purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847611ac0e92475580dda6e7094caee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.508043   1.478239  \n",
      "    1      1.516212   1.478237                               \n",
      "    2      1.509087   1.477966                               \n",
      "    3      1.5136     1.47826                                \n",
      "    4      1.514901   1.478269                               \n",
      "    5      1.511054   1.478272                               \n",
      "    6      1.516114   1.477989                               \n",
      "    7      1.514396   1.478229                               \n",
      "    8      1.516596   1.478221                               \n",
      "    9      1.51395    1.478262                               \n",
      "    10     1.509833   1.478262                               \n",
      "    11     1.516289   1.478503                               \n",
      "    12     1.51282    1.478275                               \n",
      "    13     1.515748   1.478236                               \n",
      "    14     1.512431   1.478237                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.47824])]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}') # for saving the model by taking snapshots\n",
    "cb = [CosAnneal(lo, #the layer optimzer which will allow learning rate to be changed\n",
    "                len(md.trn_dl), # the epoch \n",
    "                cycle_mult=2, \n",
    "                on_cycle_end=on_end)] # optional, save model\n",
    "                                      # save the model automatically at each cycle\n",
    "fit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e87e6fb0bfc44bcae803abdb7f6b787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=63), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                               \n",
      "    0      1.269275   1.338181  \n",
      "    1      1.267399   1.336441                               \n",
      "    2      1.264895   1.336206                               \n",
      "    3      1.270371   1.335448                               \n",
      "    4      1.257494   1.333378                               \n",
      "    5      1.256948   1.332566                               \n",
      "    6      1.250793   1.332374                               \n",
      "    7      1.252318   1.332538                               \n",
      "    8      1.248725   1.330108                               \n",
      "    9      1.24685    1.329169                               \n",
      "    10     1.233719   1.3269                                 \n",
      "    11     1.235617   1.327991                               \n",
      "    12     1.232613   1.326995                               \n",
      "    13     1.227421   1.326652                               \n",
      "    14     1.220362   1.326627                               \n",
      "    15     1.229938   1.328776                               \n",
      "    16     1.222617   1.327354                               \n",
      "    17     1.217062   1.326442                               \n",
      "    18     1.216994   1.326485                               \n",
      "    19     1.209114   1.326264                               \n",
      "    20     1.205535   1.32602                                \n",
      "    21     1.200072   1.326443                               \n",
      "    22     1.197341   1.326889                               \n",
      "    23     1.193682   1.326785                               \n",
      "    24     1.180925   1.327128                               \n",
      "    25     1.17845    1.327238                               \n",
      "    26     1.179032   1.327916                               \n",
      "    27     1.171818   1.327534                               \n",
      "    28     1.171153   1.327373                               \n",
      "    29     1.171154   1.327225                               \n",
      "    30     1.17021    1.32736                                \n",
      "    31     1.172225   1.327237                               \n",
      "    32     1.182178   1.329169                               \n",
      "    33     1.177001   1.329082                               \n",
      "    34     1.175986   1.331076                               \n",
      "    35     1.163246   1.331232                               \n",
      "    36     1.160151   1.331888                               \n",
      "    37     1.15015    1.334106                               \n",
      "    38     1.147847   1.335158                               \n",
      "    39     1.139302   1.337186                               \n",
      "    40     1.136306   1.338319                               \n",
      "    41     1.125047   1.340353                               \n",
      "    42     1.127392   1.342572                               \n",
      "    43     1.121271   1.344335                               \n",
      "    44     1.108357   1.345382                               \n",
      "    45     1.10461    1.346                                  \n",
      "    46     1.094381   1.348688                               \n",
      "    47     1.098723   1.349089                               \n",
      "    48     1.08452    1.35025                                \n",
      "    49     1.089121   1.35353                                \n",
      "    50     1.085171   1.353823                               \n",
      "    51     1.069873   1.355138                               \n",
      "    52     1.072453   1.355554                               \n",
      "    53     1.073993   1.356977                               \n",
      "    54     1.070491   1.359324                               \n",
      "    55     1.062396   1.35859                                \n",
      "    56     1.063127   1.359364                               \n",
      "    57     1.067074   1.359261                               \n",
      "    58     1.056765   1.359633                               \n",
      "    59     1.0598     1.359599                               \n",
      "    60     1.058262   1.359547                               \n",
      "    61     1.049319   1.359369                               \n",
      "    62     1.058478   1.359334                               \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.35933])]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**6-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    r = torch.multinomial(p[-1].exp(), 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for those not reealsimace; sacrification, for a spirits as power, is precogning divopicatedly required aloneas we were learn: i spirits. the stoods that our so findoming free,there arount forin just in thingand defor is for the who west\" and called too moralal being)?224. a delight! at the all because of other the biddest of a personal, andseem the miknows.\" it work of raseciraces and ctol--largid in this\n"
     ]
    }
   ],
   "source": [
    "print(get_next_n('for thos', 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "216px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
